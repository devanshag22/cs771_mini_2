{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-26T09:12:01.529951Z",
     "iopub.status.busy": "2024-11-26T09:12:01.529596Z",
     "iopub.status.idle": "2024-11-26T09:40:19.730470Z",
     "shell.execute_reply": "2024-11-26T09:40:19.728927Z",
     "shell.execute_reply.started": "2024-11-26T09:12:01.529918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import ConvNeXt_Tiny_Weights\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Load data functions\n",
    "def load_data(file_path):\n",
    "    \"\"\"Loads the labeled dataset (D1).\"\"\"\n",
    "    data_dict = torch.load(file_path)\n",
    "    return data_dict['data'], data_dict['targets']\n",
    "\n",
    "def load_data_unlabeled(file_path):\n",
    "    \"\"\"Loads the unlabeled datasets (D2, D3, ..., D10).\"\"\"\n",
    "    data_dict = torch.load(file_path)\n",
    "    return data_dict['data']  # Only 'data', no 'targets'\n",
    "\n",
    "# 2. Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ConvNeXt expects 224x224\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet stats\n",
    "])\n",
    "\n",
    "def apply_transforms(data):\n",
    "    \"\"\"Apply transformations to a batch of images.\"\"\"\n",
    "    data_transformed = torch.stack([transform(image / 255.0) for image in data])\n",
    "    return data_transformed\n",
    "\n",
    "# 3. ConvNeXt Embedder\n",
    "class ConvNeXtEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNeXtEmbedder, self).__init__()\n",
    "        convnext = models.convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
    "        self.feature_extractor = nn.Sequential(*list(convnext.children())[:-2])  # Remove classification head\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x.reshape(x.size(0), -1)  # Flatten using reshape\n",
    "\n",
    "embedder = ConvNeXtEmbedder().eval()\n",
    "\n",
    "# 4. Extract embeddings\n",
    "def extract_embeddings(data, embedder, batch_size=32):\n",
    "    \"\"\"Extract embeddings for a dataset using the ConvNeXt embedder.\"\"\"\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        data = data.to(device)\n",
    "        embedder = embedder.to(device)\n",
    "        for i in tqdm(range(0, len(data), batch_size), desc=\"Extracting Embeddings\"):\n",
    "            batch = data[i:i+batch_size]\n",
    "            embeddings.append(embedder(batch).cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# 5. Gaussian Generative Classifier (GGC)\n",
    "class GaussianGenerativeClassifier:\n",
    "    def __init__(self):\n",
    "        self.means = {}  # Mean vector for each class\n",
    "        self.covariances = {}  # Covariance matrix for each class\n",
    "        self.priors = {}  # Prior probability for each class\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the generative model by estimating the means, covariances, and priors.\n",
    "        \"\"\"\n",
    "        for cls in np.unique(y):\n",
    "            class_points = X[y == cls]\n",
    "            self.means[cls] = np.mean(class_points, axis=0)\n",
    "            self.covariances[cls] = np.cov(class_points, rowvar=False)\n",
    "            self.priors[cls] = class_points.shape[0] / X.shape[0]\n",
    "\n",
    "    def gaussian_log_density(self, x, mean, covariance):\n",
    "        \"\"\"\n",
    "        Compute the logarithm of the Gaussian density function.\n",
    "        \"\"\"\n",
    "        d = mean.shape[0]\n",
    "        diff = x - mean\n",
    "        # Regularize covariance to avoid singular matrix issues\n",
    "        regularization = 1e-6 * np.eye(d)\n",
    "        cov_inv = np.linalg.inv(covariance + regularization)\n",
    "        det_cov = np.linalg.det(covariance + regularization)\n",
    "        \n",
    "        # Log-normalization term\n",
    "        log_norm = -0.5 * (d * np.log(2 * np.pi) + np.log(det_cov))\n",
    "        # Log-exponent term\n",
    "        log_exponent = -0.5 * diff.T @ cov_inv @ diff\n",
    "        return log_norm + log_exponent\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given data using log-densities.\n",
    "        \"\"\"\n",
    "        preds = []\n",
    "        for x in tqdm(X, desc=\"Predicting Labels\"):\n",
    "            class_probs = {}\n",
    "            for cls in self.means.keys():\n",
    "                log_likelihood = self.gaussian_log_density(x, self.means[cls], self.covariances[cls])\n",
    "                log_prior = np.log(self.priors[cls])\n",
    "                class_probs[cls] = log_likelihood + log_prior\n",
    "            preds.append(max(class_probs, key=class_probs.get))\n",
    "        return np.array(preds)\n",
    "\n",
    "# 6. PCA Wrapper for Dimensionality Reduction\n",
    "class PCAEmbedder:\n",
    "    def __init__(self, n_components=50):\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "\n",
    "    def fit_transform(self, embeddings):\n",
    "        \"\"\"Fit PCA on the embeddings and transform them.\"\"\"\n",
    "        return self.pca.fit_transform(embeddings)\n",
    "\n",
    "    def transform(self, embeddings):\n",
    "        \"\"\"Transform embeddings using fitted PCA.\"\"\"\n",
    "        return self.pca.transform(embeddings)\n",
    "\n",
    "# Initialize variables\n",
    "train_datasets = [f\"/kaggle/input/cs771-mp2/dataset/part_one_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)] #change path to the location of the dataset\n",
    "eval_datasets = [f\"/kaggle/input/cs771-mp2/dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)] #change path to the location of the dataset\n",
    "models = []\n",
    "accuracy_matrix = np.zeros((10, 10))  # Rows: Models f1 to f10, Columns: Held-out datasets D̂1 to D̂10\n",
    "pca_embedder = PCAEmbedder(n_components=50)\n",
    "\n",
    "all_train_embeddings = []\n",
    "all_train_labels = []\n",
    "\n",
    "# Train f1 with PCA\n",
    "print(\"Training model f1...\")\n",
    "train_data, train_targets = load_data(train_datasets[0])\n",
    "train_data = apply_transforms(torch.tensor(train_data).permute(0, 3, 1, 2))\n",
    "train_targets = torch.tensor(train_targets)\n",
    "\n",
    "train_embeddings = extract_embeddings(train_data, embedder)\n",
    "all_train_embeddings.append(train_embeddings)\n",
    "all_train_labels.append(train_targets.numpy())\n",
    "\n",
    "reduced_train_embeddings = pca_embedder.fit_transform(np.vstack(all_train_embeddings))\n",
    "\n",
    "model_f1 = GaussianGenerativeClassifier()\n",
    "model_f1.fit(reduced_train_embeddings, np.hstack(all_train_labels))\n",
    "models.append(model_f1)\n",
    "\n",
    "for j, eval_file in enumerate(eval_datasets[:1]):\n",
    "    eval_data, eval_targets = load_data(eval_file)\n",
    "    eval_data = apply_transforms(torch.tensor(eval_data).permute(0, 3, 1, 2))\n",
    "    eval_embeddings = extract_embeddings(eval_data, embedder)\n",
    "    reduced_eval_embeddings = pca_embedder.transform(eval_embeddings)\n",
    "    eval_targets = torch.tensor(eval_targets)\n",
    "\n",
    "    predictions = model_f1.predict(reduced_eval_embeddings)\n",
    "    accuracy = accuracy_score(eval_targets.numpy(), predictions)\n",
    "    accuracy_matrix[0, j] = accuracy\n",
    "    print(f\"Accuracy of f1 on D̂{j+1}: {accuracy:.4f}\")\n",
    "\n",
    "# Train f2 to f10 with updated PCA\n",
    "for i in range(1, 10):\n",
    "    print(f\"Training model f{i+1}...\")\n",
    "    train_data = load_data_unlabeled(train_datasets[i])\n",
    "    train_data = apply_transforms(torch.tensor(train_data).permute(0, 3, 1, 2))\n",
    "\n",
    "    current_model = models[-1]\n",
    "    train_embeddings = extract_embeddings(train_data, embedder)\n",
    "    reduced_train_embeddings = pca_embedder.transform(train_embeddings)\n",
    "    predicted_labels = current_model.predict(reduced_train_embeddings)\n",
    "\n",
    "    all_train_embeddings.append(train_embeddings)\n",
    "    all_train_labels.append(predicted_labels)\n",
    "\n",
    "    all_embeddings_stack = np.vstack(all_train_embeddings)\n",
    "    pca_embedder = PCAEmbedder(n_components=50)\n",
    "    reduced_all_embeddings = pca_embedder.fit_transform(all_embeddings_stack)\n",
    "\n",
    "    updated_model = GaussianGenerativeClassifier()\n",
    "    updated_model.fit(reduced_all_embeddings, np.hstack(all_train_labels))\n",
    "    models.append(updated_model)\n",
    "\n",
    "    for j, eval_file in enumerate(eval_datasets[:i + 1]):\n",
    "        eval_data, eval_targets = load_data(eval_file)\n",
    "        eval_data = apply_transforms(torch.tensor(eval_data).permute(0, 3, 1, 2))\n",
    "        eval_embeddings = extract_embeddings(eval_data, embedder)\n",
    "        reduced_eval_embeddings = pca_embedder.transform(eval_embeddings)\n",
    "        eval_targets = torch.tensor(eval_targets)\n",
    "\n",
    "        predictions = updated_model.predict(reduced_eval_embeddings)\n",
    "        accuracy = accuracy_score(eval_targets.numpy(), predictions)\n",
    "        accuracy_matrix[i, j] = accuracy\n",
    "        print(f\"Accuracy of f{i+1} on D̂{j+1}: {accuracy:.4f}\")\n",
    "\n",
    "# Print final accuracy matrix\n",
    "print(\"Final Accuracy Matrix (Models vs Held-out Datasets):\")\n",
    "print(accuracy_matrix)\n",
    "# Save the accuracy matrix as a CSV file\n",
    "np.savetxt(\"accuracy_matrix_part_one.csv\", accuracy_matrix, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-26T09:51:06.127130Z",
     "iopub.status.busy": "2024-11-26T09:51:06.126385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# File paths for part two datasets D11 to D20 (unlabeled)\n",
    "train_datasets_part_two = [f\"/kaggle/input/cs771-mp2/dataset/part_two_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)] #change path to the location of the dataset\n",
    "eval_datasets_part_two = [f\"/kaggle/input/cs771-mp2/dataset/part_two_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)] #change path to the location of the dataset\n",
    "\n",
    "# Combine all train and evaluation datasets (D1 to D20)\n",
    "all_train_datasets = train_datasets + train_datasets_part_two\n",
    "all_eval_datasets = eval_datasets + eval_datasets_part_two\n",
    "\n",
    "# Accuracy matrix of size 10x20 for f11 to f20 vs D̂1 to D̂20\n",
    "accuracy_matrix_part_two = np.zeros((10, 20))\n",
    "\n",
    "# Initialize PCA embedder for dimensionality reduction\n",
    "pca_embedder_part_two = PCAEmbedder(n_components=50)\n",
    "\n",
    "# Accumulate embeddings and labels from Task 1\n",
    "all_train_embeddings_part_two = list(np.vstack(all_train_embeddings))  # Embeddings from Task 1\n",
    "all_train_labels_part_two = list(np.hstack(all_train_labels))  # Labels from Task 1\n",
    "\n",
    "# Sequential training for D11 to D20\n",
    "for i in range(10, 20):  # Start from D11 (index 10)\n",
    "    print(f\"Training model f{i+1}...\")\n",
    "\n",
    "    # Load the unlabeled dataset D11 to D20\n",
    "    train_data = load_data_unlabeled(all_train_datasets[i])  # Unlabeled datasets\n",
    "    train_data = apply_transforms(torch.tensor(train_data).permute(0, 3, 1, 2))\n",
    "\n",
    "    # Use the last trained model (e.g., f10 for D11) to predict pseudo-labels\n",
    "    current_model = models[-1]  # Start with f10\n",
    "    train_embeddings = extract_embeddings(train_data, embedder)\n",
    "\n",
    "    # Add new embeddings to accumulated embeddings\n",
    "    all_train_embeddings_part_two.append(train_embeddings)\n",
    "\n",
    "    # Refit PCA with all embeddings seen so far\n",
    "    pca_embedder_part_two = PCAEmbedder(n_components=50)  # Reset PCA embedder\n",
    "    reduced_all_embeddings = pca_embedder_part_two.fit_transform(np.vstack(all_train_embeddings_part_two))\n",
    "\n",
    "    # Transform the current dataset using the updated PCA\n",
    "    reduced_train_embeddings = reduced_all_embeddings[-len(train_embeddings):]  # Get reduced embeddings for current dataset\n",
    "\n",
    "    # Predict pseudo-labels\n",
    "    predicted_labels = current_model.predict(reduced_train_embeddings)\n",
    "\n",
    "    # Add predicted labels to accumulated labels\n",
    "    all_train_labels_part_two.append(predicted_labels)\n",
    "\n",
    "    # Train the updated model with pseudo-labeled data\n",
    "    updated_model = GaussianGenerativeClassifier()\n",
    "    updated_model.fit(reduced_all_embeddings, np.hstack(all_train_labels_part_two))  # Fit with all reduced embeddings and labels\n",
    "    models.append(updated_model)\n",
    "\n",
    "    # Evaluate the updated model on all held-out datasets up to the current one\n",
    "    for j, eval_file in enumerate(all_eval_datasets[:i + 1]):  # Evaluate on D̂1 to D̂i+1\n",
    "        eval_data, eval_targets = load_data(eval_file)\n",
    "        eval_data = apply_transforms(torch.tensor(eval_data).permute(0, 3, 1, 2))\n",
    "        eval_embeddings = extract_embeddings(eval_data, embedder)\n",
    "\n",
    "        # Transform evaluation data using updated PCA\n",
    "        reduced_eval_embeddings = pca_embedder_part_two.transform(eval_embeddings)\n",
    "\n",
    "        # Predict and calculate accuracy\n",
    "        predictions = updated_model.predict(reduced_eval_embeddings)\n",
    "        accuracy = accuracy_score(eval_targets, predictions)\n",
    "        accuracy_matrix_part_two[i - 10, j] = accuracy  # Adjusted indexing for f11 to f20\n",
    "        print(f\"Accuracy of f{i+1} on D̂{j+1}: {accuracy:.4f}\")\n",
    "\n",
    "# Print final accuracy matrix for Task 2\n",
    "print(\"Final Accuracy Matrix for Models f11 to f20 vs Held-out Datasets D̂1 to D̂20:\")\n",
    "print(accuracy_matrix_part_two)\n",
    "# Save the accuracy matrix as a CSV file\n",
    "np.savetxt(\"accuracy_matrix_part_two.csv\", accuracy_matrix_part_two, delimiter=\",\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6143059,
     "sourceId": 9982861,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
